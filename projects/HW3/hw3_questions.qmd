---
title: "Multinomial Logit Model"
author: "Nadifa Hossain"
date: today
---


This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm. 


## 1. Likelihood for the Multi-nomial Logit (MNL) Model

Suppose we have $i=1,\ldots,n$ consumers who each select exactly one product $j$ from a set of $J$ products. The outcome variable is the identity of the product chosen $y_i \in \{1, \ldots, J\}$ or equivalently a vector of $J-1$ zeros and $1$ one, where the $1$ indicates the selected product. For example, if the third product was chosen out of 3 products, then either $y=3$ or $y=(0,0,1)$ depending on how we want to represent it. Suppose also that we have a vector of data on each product $x_j$ (eg, brand, price, etc.). 

We model the consumer's decision as the selection of the product that provides the most utility, and we'll specify the utility function as a linear function of the product characteristics:

$$ U_{ij} = x_j'\beta + \epsilon_{ij} $$

where $\epsilon_{ij}$ is an i.i.d. extreme value error term. 

The choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer $i$ chooses product $j$:

$$ \mathbb{P}_i(j) = \frac{e^{x_j'\beta}}{\sum_{k=1}^Je^{x_k'\beta}} $$

For example, if there are 3 products, the probability that consumer $i$ chooses product 3 is:

$$ \mathbb{P}_i(3) = \frac{e^{x_3'\beta}}{e^{x_1'\beta} + e^{x_2'\beta} + e^{x_3'\beta}} $$

A clever way to write the individual likelihood function for consumer $i$ is the product of the $J$ probabilities, each raised to the power of an indicator variable ($\delta_{ij}$) that indicates the chosen product:

$$ L_i(\beta) = \prod_{j=1}^J \mathbb{P}_i(j)^{\delta_{ij}} = \mathbb{P}_i(1)^{\delta_{i1}} \times \ldots \times \mathbb{P}_i(J)^{\delta_{iJ}}$$

Notice that if the consumer selected product $j=3$, then $\delta_{i3}=1$ while $\delta_{i1}=\delta_{i2}=0$ and the likelihood is:

$$ L_i(\beta) = \mathbb{P}_i(1)^0 \times \mathbb{P}_i(2)^0 \times \mathbb{P}_i(3)^1 = \mathbb{P}_i(3) = \frac{e^{x_3'\beta}}{\sum_{k=1}^3e^{x_k'\beta}} $$

The joint likelihood (across all consumers) is the product of the $n$ individual likelihoods:

$$ L_n(\beta) = \prod_{i=1}^n L_i(\beta) = \prod_{i=1}^n \prod_{j=1}^J \mathbb{P}_i(j)^{\delta_{ij}} $$

And the joint log-likelihood function is:

$$ \ell_n(\beta) = \sum_{i=1}^n \sum_{j=1}^J \delta_{ij} \log(\mathbb{P}_i(j)) $$



## 2. Simulate Conjoint Data

We will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a "no choice" option; each simulated respondent must select one of the 3 alternatives. 

Each alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from \$4 to \$32 in increments of \$4.

The part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer $i$ for hypothethical streaming service $j$ is 

$$
u_{ij} = (1 \times Netflix_j) + (0.5 \times Prime_j) + (-0.8*Ads_j) - 0.1\times Price_j + \varepsilon_{ij}
$$

where the variables are binary indicators and $\varepsilon$ is Type 1 Extreme Value (ie, Gumble) distributed.

The following code provides the simulation of the conjoint data.

:::: {.callout-note collapse="true"}

```{python}
import numpy as np
import pandas as pd

# Set seed for reproducibility
np.random.seed(123)

# Define attributes
brands = ['N', 'P', 'H']  # Netflix, Prime, Hulu
ads = ['Yes', 'No']
prices = np.arange(8, 33, 4)  # 8 to 32 inclusive

# Generate all possible profiles
import itertools
profiles = pd.DataFrame(list(itertools.product(brands, ads, prices)), columns=['brand', 'ad', 'price'])
m = profiles.shape[0]

# Define part-worth utilities
b_util = {'N': 1.0, 'P': 0.5, 'H': 0.0}
a_util = {'Yes': -0.8, 'No': 0.0}
p_util = lambda p: -0.1 * p

# Simulation settings
n_peeps = 100
n_tasks = 10
n_alts = 3

# Function to simulate one respondent's data
def sim_one(resp_id):
    tasks = []
    for task in range(1, n_tasks + 1):
        sampled = profiles.sample(n=n_alts, replace=False).copy()
        sampled['resp'] = resp_id
        sampled['task'] = task

        # Compute deterministic utility
        sampled['v'] = (
            sampled['brand'].map(b_util) +
            sampled['ad'].map(a_util) +
            p_util(sampled['price'])
        ).round(10)

        # Add Gumbel noise (Type I extreme value)
        sampled['e'] = -np.log(-np.log(np.random.uniform(size=n_alts)))
        sampled['u'] = sampled['v'] + sampled['e']

        # Identify chosen alternative
        sampled['choice'] = (sampled['u'] == sampled['u'].max()).astype(int)

        tasks.append(sampled[['resp', 'task', 'brand', 'ad', 'price', 'choice']])
    return pd.concat(tasks, ignore_index=True)

# Simulate data for all respondents
conjoint_data = pd.concat([sim_one(i) for i in range(1, n_peeps + 1)], ignore_index=True)

# View first few rows
conjoint_data.head()
```

::::



## 3. Preparing the Data for Estimation

The "hard part" of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer $i$, covariate $k$, and product $j$) instead of the typical 2 dimensions for cross-sectional regression models (consumer $i$ and covariate $k$). The fact that each task for each respondent has the same number of alternatives (3) helps.  In addition, we need to convert the categorical variables for brand and ads into binary variables.

```{python}
import pandas as pd

# Load the conjoint data
conjoint_data = pd.read_csv("conjoint_data.csv")

# One-hot encode categorical variables (Hulu and Ad-Free as reference levels)
conjoint_prepped = pd.get_dummies(conjoint_data, columns=["brand", "ad"], drop_first=True)

# Sort the data to preserve the order by respondent and task
conjoint_prepped = conjoint_prepped.sort_values(by=["resp", "task"]).reset_index(drop=True)

# Create the design matrix (X) and the target vector (y)
X = conjoint_prepped[["brand_N", "brand_P", "ad_Yes", "price"]]
y = conjoint_prepped["choice"]

# Optionally combine X and y with respondent/task info for review
conjoint_ready = pd.concat([conjoint_prepped[["resp", "task"]], X, y], axis=1)

# Display first few rows
conjoint_ready.head()
```

## 4. Estimation via Maximum Likelihood

```{python}
import numpy as np
import pandas as pd
from scipy.optimize import minimize

# Step 1: Define the MNL log-likelihood function
def mnl_log_likelihood(beta, X, y, group_size=3):
    beta = np.asarray(beta, dtype=np.float64)
    X_np = np.asarray(X, dtype=np.float64)
    y_np = np.asarray(y, dtype=np.int64)

    # Compute utilities and reshape into (n_tasks, n_alternatives)
    Xb = X_np @ beta
    Xb = Xb.reshape(-1, group_size)
    y_np = y_np.reshape(-1, group_size)

    # Compute softmax probabilities
    exp_Xb = np.exp(Xb)
    probs = exp_Xb / np.sum(exp_Xb, axis=1, keepdims=True)

    # Compute log-likelihood (only for chosen alternatives)
    chosen_probs = np.sum(probs * y_np, axis=1)
    log_likelihood = np.sum(np.log(chosen_probs))

    return -log_likelihood  # for minimization
```

```{python}
# Step 2: Run MLE using scipy.optimize.minimize
init_params = np.zeros(X.shape[1])  # starting at 0s

result = minimize(
    mnl_log_likelihood,
    init_params,
    args=(X, y),
    method='BFGS',
    options={'disp': True}
)

# Extract estimates
beta_hat = result.x
hessian_inv = result.hess_inv  # Hessian inverse for variance
```

```{python}
# Step 3: Standard errors and confidence intervals
se = np.sqrt(np.diag(hessian_inv))
z = 1.96  # for 95% CI
ci_lower = beta_hat - z * se
ci_upper = beta_hat + z * se

# Step 4: Present results in a DataFrame
param_names = ['beta_netflix', 'beta_prime', 'beta_ads', 'beta_price']
results = pd.DataFrame({
    'Parameter': param_names,
    'Estimate': beta_hat,
    'Std. Error': se,
    '95% CI Lower': ci_lower,
    '95% CI Upper': ci_upper
})

results
```

## 5. Estimation via Bayesian Methods

```{python}
import numpy as np
import pandas as pd

# Define log-prior function
def log_prior(beta):
    # Priors: N(0,5) for first 3 (binary vars), N(0,1) for price
    binary_log_priors = -0.5 * (beta[:3] ** 2) / 5 - 0.5 * np.log(2 * np.pi * 5)
    price_log_prior = -0.5 * (beta[3] ** 2) / 1 - 0.5 * np.log(2 * np.pi * 1)
    return np.sum(binary_log_priors) + price_log_prior

# Log-posterior function = log-likelihood + log-prior
def log_posterior(beta, X, y, group_size=3):
    return -mnl_log_likelihood(beta, X, y, group_size) + log_prior(beta)

# Metropolis-Hastings MCMC
def metropolis_sampler(log_post_func, X, y, n_iter=11000, burn_in=1000, proposal_sd=[0.05, 0.05, 0.05, 0.005]):
    d = X.shape[1]
    samples = np.zeros((n_iter, d))
    current_beta = np.zeros(d)
    current_log_post = log_post_func(current_beta, X, y)

    for i in range(n_iter):
        # Propose new beta from independent normal proposals
        proposal = current_beta + np.random.normal(loc=0, scale=proposal_sd)
        proposal_log_post = log_post_func(proposal, X, y)

        # Acceptance probability
        accept_prob = np.exp(proposal_log_post - current_log_post)
        if np.random.rand() < accept_prob:
            current_beta = proposal
            current_log_post = proposal_log_post

        samples[i] = current_beta

    return samples[burn_in:]  # remove burn-in
```

```{python}
# Run MCMC
posterior_samples = metropolis_sampler(log_posterior, X, y, n_iter=11000, burn_in=1000)

# Summarize posterior
param_names = ['beta_netflix', 'beta_prime', 'beta_ads', 'beta_price']
summary = pd.DataFrame({
    'Parameter': param_names,
    'Mean': posterior_samples.mean(axis=0),
    'Std. Dev.': posterior_samples.std(axis=0),
    '2.5%': np.percentile(posterior_samples, 2.5, axis=0),
    '97.5%': np.percentile(posterior_samples, 97.5, axis=0)
})
summary
```

```{python}
import matplotlib.pyplot as plt

# Select parameter index (0: beta_netflix, 1: beta_prime, 2: beta_ads, 3: beta_price)
param_index = 3
param_name = 'beta_price'
samples = posterior_samples[:, param_index]

# Trace plot
plt.figure(figsize=(10, 4))
plt.plot(samples)
plt.title(f"Trace Plot for {param_name}")
plt.xlabel("Iteration")
plt.ylabel("Parameter Value")
plt.show()
```

```{python}
# Histogram of posterior samples
plt.figure(figsize=(8, 6))
plt.hist(samples, bins=40, edgecolor='black', density=True)
plt.title(f"Posterior Distribution for {param_name}")
plt.xlabel("Parameter Value")
plt.ylabel("Density")
plt.show()
```


```{python}
# Bayesian summary
bayes_summary = pd.DataFrame({
    'Parameter': ['beta_netflix', 'beta_prime', 'beta_ads', 'beta_price'],
    'Bayes Mean': posterior_samples.mean(axis=0),
    'Bayes Std. Dev.': posterior_samples.std(axis=0),
    'Bayes 2.5%': np.percentile(posterior_samples, 2.5, axis=0),
    'Bayes 97.5%': np.percentile(posterior_samples, 97.5, axis=0)
})

# Merge with MLE results
comparison = results.merge(bayes_summary, on='Parameter')

# Show comparison
comparison[['Parameter', 'Estimate', 'Std. Error', '95% CI Lower', '95% CI Upper',
            'Bayes Mean', 'Bayes Std. Dev.', 'Bayes 2.5%', 'Bayes 97.5%']]
```


## 6. Discussion

If we had not simulated the data and instead estimated the model from real survey responses, we would interpret the parameter estimates as reflecting **consumer preferences** for different streaming service attributes.

- The fact that $\beta_\text{Netflix} > \beta_\text{Prime}$ implies that, on average, respondents prefer **Netflix over Amazon Prime** when all other attributes (ads and price) are held constant. This aligns with common perceptions of Netflix as a more dominant or desirable streaming brand.

- A **negative coefficient for price** ($\beta_\text{price} < 0$) makes intuitive and economic sense: as price increases, utility (and hence the probability of choosing an option) decreases. This confirms **price sensitivity**, a key aspect of consumer choice modeling.

- The magnitude of $\beta_\text{ads}$ (likely negative) reflects disutility from included advertisements. A larger negative value would suggest that respondents strongly prefer **ad-free experiences**.

Overall, the signs and relative sizes of the parameters are **consistent with rational consumer behavior**. The comparison between the MLE and Bayesian results shows that both approaches yield similar inference, with Bayesian methods also providing full posterior distributions for the parameters.

## Additional Discussion: Extending to a Multi-Level Model

To simulate data from — and estimate parameters of — a **multi-level (random parameter or hierarchical) MNL model**, the key conceptual change is to allow **individual-level heterogeneity** in preferences.

### Simulation Changes:
In the current model, all respondents share the same coefficients (`β`) for attributes. To simulate data from a hierarchical model, we would:
- Draw each respondent's coefficients from a **population distribution**.  
  For example:
  $$
  \beta_i \sim \mathcal{N}(\mu, \Sigma)
  $$
  where $\mu$ is the average preference vector across all respondents, and $\Sigma$ is the covariance matrix capturing variability in preferences across individuals.
- Then simulate each respondent’s choices using their **own** $\beta_i$.

### Estimation Changes:
To estimate such a model:
- You can no longer rely on a single likelihood function over one shared $\beta$.
- Instead, use **Bayesian hierarchical modeling**, where:
  - Each respondent has their own $\beta_i$.
  - The $\beta_i$'s are modeled as coming from a population-level distribution with unknown hyperparameters $(\mu, \Sigma)$.
- Estimation typically requires **MCMC sampling** methods such as **Gibbs sampling** or **Hamiltonian Monte Carlo** (e.g., via PyMC, Stan, or NUTS samplers).

This hierarchical structure allows the model to better fit **real-world conjoint data**, where preferences vary significantly across individuals.











