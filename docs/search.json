[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nadifa Hossain",
    "section": "",
    "text": "About Me:\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "hw1_questions.html",
    "href": "hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "hw1_questions.html#introduction",
    "href": "hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "hw1_questions.html#data",
    "href": "hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\ntodo: Read the data into R/Python and describe the data\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\ntodo: test a few variables other than the key outcome variables (for example, test months since last donation) to see if the treatment and control groups are statistically significantly different at the 95% confidence level. Do each as a t-test and separately as a linear regression, and confirm you get the exact same results from both methods. When doing a t-test, use the formula in the class slides. When doing the linear regression, regress for example mrm2 on treatment and look at the estimated coefficient on the treatment variable. It might be helpful to compare parts of your analysis to Table 1 in the paper. Be sure to comment on your results (hint: why is Table 1 included in the paper)."
  },
  {
    "objectID": "hw1_questions.html#experimental-results",
    "href": "hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\ntodo: make a barplot with two bars. Each bar is the proportion of people who donated. One bar for treatment and one bar for control.\ntodo: run a t-test between the treatment and control groups on the binary outcome of whether any charitable donation was made. Also run a bivariate linear regression that demonstrates the same finding. (It may help to confirm your calculations match Table 2a Panel A.) Report your statistical results and interpret them in the context of the experiment (e.g., if you found a difference with a small p-value or that was statistically significant at some threshold, what have you learned about human behavior? Use mostly English words, not numbers or stats, to explain your finding.)\ntodo: run a probit regression where the outcome variable is whether any charitable donation was made and the explanatory variable is assignment to treatment or control. Confirm that your results replicate Table 3 column 1 in the paper.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\ntodo: Use a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not. For example, does the 2:1 match rate lead increase the likelihood that someone donates as compared to the 1:1 match rate? Do your results support the “figures suggest” comment the authors make on page 8?\ntodo: Assess the same issue using a regression. Specifically, create the variable ratio1 then regress gave on ratio1, ratio2, and ratio3 (or alternatively, regress gave on the categorical variable ratio). Interpret the coefficients and their statistical precision.\ntodo: Calculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios. Do this directly from the data, and do it by computing the differences in the fitted coefficients of the previous regression. what do you conclude regarding the effectiveness of different sizes of matched donations?\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\ntodo: Calculate a t-test or run a bivariate linear regression of the donation amount on the treatment status. What do we learn from doing this analysis?\ntodo: now limit the data to just people who made a donation and repeat the previous analysis. This regression allows you to analyze how much respondents donate conditional on donating some positive amount. Interpret the regression coefficients – what did we learn? Does the treatment coefficient have a causal interpretation?\ntodo: Make two plot: one for the treatment group and one for the control. Each plot should be a histogram of the donation amounts only among people who donated. Add a red vertical bar or some other annotation to indicate the sample average for each plot."
  },
  {
    "objectID": "hw1_questions.html#simulation-experiment",
    "href": "hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nto do: Make a plot like those on slide 43 from our first class and explain the plot to the reader. To do this, you will simulate 100,00 draws from the control distribution and 10,000 draws from the treatment distribution. You’ll then calculate a vector of 10,000 differences, and then you’ll plot the cumulative average of that vector of differences. Comment on whether the cumulative average approaches the true difference in means.\n\n\nCentral Limit Theorem\nto do: Make 4 histograms like those on slide 44 from our first class at sample sizes 50, 200, 500, and 1000 and explain these plots to the reader. To do this for a sample size of e.g. 50, take 50 draws from each of the control and treatment distributions, and calculate the average difference between those draws. Then repeat that process 999 more times so that you have 1000 averages. Plot the histogram of those averages. Comment on whether zero is in the “middle” of the distribution or whether it’s in the “tail.”"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "Karlan & List 2007 Data Description\n\n\n\n\nNadifa Hossain\nApr 23, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/HW1/hw1_questions.html",
    "href": "projects/HW1/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nTo explore how different “price” signals affect charitable giving, Dean Karlan and John List conducted a large-scale natural field experiment involving over 50,000 prior donors to a U.S.-based liberal nonprofit. The experiment aimed to test whether and how matching grant offers—a common fundraising tactic—alter donor behavior.\nParticipants were randomly assigned to receive one of several different types of fundraising letters. The control group received a standard letter without any mention of a matching grant. The treatment group received a letter with an announcement that a “concerned fellow member” would match their donation. Within the treatment group, further randomization varied three key features:\nMatching ratio: The letters promised either a $1:$1, $2:$1, or $3:$1 match.\nMaximum match amount: The match was capped at either $25,000, $50,000, $100,000, or left unstated.\nSuggested donation amounts: Each participant saw one of three ask amounts—equal to, 1.25×, or 1.5× their highest previous donation.\nThis design allowed the researchers to test both main effects and interaction effects of price (via match ratio), perceived value (via cap), and anchoring (via ask amounts).\nKey Findings:\n\nThe presence of a matching grant increased both the response rate (probability of donating) and the average donation.\nMerely including a match offer increased revenue per solicitation by 19% and the response rate by 22%.\nSurprisingly, higher match ratios ($2:$1 or $3:$1) did not produce better results than the $1:$1 ratio.\nGeographic political context mattered: Donors in “red states” (those that voted for George W. Bush in 2004) were significantly more responsive to the match offer than those in “blue states.”\nNo significant differences were found across different match cap amounts or suggested donation levels.\n\nThis study, published in the American Economic Review (2007), is notable for being one of the first real-world randomized trials testing economic theories of charitable giving on the “demand side”—how donor behavior responds to perceived price changes, framing effects, and social signals in a natural setting.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/HW1/hw1_questions.html#introduction",
    "href": "projects/HW1/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nTo explore how different “price” signals affect charitable giving, Dean Karlan and John List conducted a large-scale natural field experiment involving over 50,000 prior donors to a U.S.-based liberal nonprofit. The experiment aimed to test whether and how matching grant offers—a common fundraising tactic—alter donor behavior.\nParticipants were randomly assigned to receive one of several different types of fundraising letters. The control group received a standard letter without any mention of a matching grant. The treatment group received a letter with an announcement that a “concerned fellow member” would match their donation. Within the treatment group, further randomization varied three key features:\nMatching ratio: The letters promised either a $1:$1, $2:$1, or $3:$1 match.\nMaximum match amount: The match was capped at either $25,000, $50,000, $100,000, or left unstated.\nSuggested donation amounts: Each participant saw one of three ask amounts—equal to, 1.25×, or 1.5× their highest previous donation.\nThis design allowed the researchers to test both main effects and interaction effects of price (via match ratio), perceived value (via cap), and anchoring (via ask amounts).\nKey Findings:\n\nThe presence of a matching grant increased both the response rate (probability of donating) and the average donation.\nMerely including a match offer increased revenue per solicitation by 19% and the response rate by 22%.\nSurprisingly, higher match ratios ($2:$1 or $3:$1) did not produce better results than the $1:$1 ratio.\nGeographic political context mattered: Donors in “red states” (those that voted for George W. Bush in 2004) were significantly more responsive to the match offer than those in “blue states.”\nNo significant differences were found across different match cap amounts or suggested donation levels.\n\nThis study, published in the American Economic Review (2007), is notable for being one of the first real-world randomized trials testing economic theories of charitable giving on the “demand side”—how donor behavior responds to perceived price changes, framing effects, and social signals in a natural setting.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/HW1/hw1_questions.html#data",
    "href": "projects/HW1/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\ntitle: “Karlan & List 2007 Data Description” format: html execute: echo: true warning: false message: false —"
  },
  {
    "objectID": "projects/HW1/hw1_questions.html#experimental-results",
    "href": "projects/HW1/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\nWe investigate whether being offered a matching grant (treatment) increased the likelihood of making a donation.\nWe’ll: - Visualize donation rates by group - Perform a Welch’s t-test and a linear regression - Run a probit regression to match results in Table 3, Column 1\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom scipy.stats import t\n\n# --- Barplot: proportion who donated by treatment group ---\ndonation_rates = df.groupby('treatment')['gave'].mean()\n\nplt.figure(figsize=(10, 6))\nplt.bar(['Control', 'Treatment'], donation_rates.values)\nplt.title('Proportion of People Who Donated by Group')\nplt.ylabel('Proportion Donated')\nplt.ylim(0, 0.03)\nplt.grid(axis='y')\nplt.show()\n\n# --- Welch's t-test manually ---\ntreat = df[df['treatment'] == 1]['gave']\ncontrol = df[df['treatment'] == 0]['gave']\n\nx1, x2 = treat.mean(), control.mean()\ns1, s2 = treat.std(ddof=1), control.std(ddof=1)\nn1, n2 = len(treat), len(control)\n\nse = np.sqrt((s1**2 / n1) + (s2**2 / n2))\nt_stat = (x1 - x2) / se\ndf_num = (s1**2 / n1 + s2**2 / n2)**2\ndf_denom = ((s1**2 / n1)**2 / (n1 - 1)) + ((s2**2 / n2)**2 / (n2 - 1))\ndf_welch = df_num / df_denom\np_val_t = 2 * t.sf(np.abs(t_stat), df_welch)\n\n# --- Linear regression ---\nlm = smf.ols('gave ~ treatment', data=df).fit()\n\n# --- Probit regression ---\nprobit = smf.probit('gave ~ treatment', data=df).fit(disp=0)\n\n# --- Output summary ---\nprint(\"=== Barplot: Proportion Donated ===\")\nprint(donation_rates)\n\nprint(\"\\n=== Welch’s t-test ===\")\nprint(f\"t-statistic = {t_stat:.3f}, p-value = {p_val_t:.4f}\")\n\nprint(\"\\n=== Linear Regression ===\")\nprint(lm.summary())\n\nprint(\"\\n=== Probit Regression ===\")\nprint(probit.summary())\n\n\n\n\n\n\n\n\n=== Barplot: Proportion Donated ===\ntreatment\n0    0.017858\n1    0.022039\nName: gave, dtype: float64\n\n=== Welch’s t-test ===\nt-statistic = 3.209, p-value = 0.0013\n\n=== Linear Regression ===\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     9.618\nDate:                Wed, 23 Apr 2025   Prob (F-statistic):            0.00193\nTime:                        16:11:57   Log-Likelihood:                 26630.\nNo. Observations:               50083   AIC:                        -5.326e+04\nDf Residuals:                   50081   BIC:                        -5.324e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.0179      0.001     16.225      0.000       0.016       0.020\ntreatment      0.0042      0.001      3.101      0.002       0.002       0.007\n==============================================================================\nOmnibus:                    59814.280   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4317152.727\nSkew:                           6.740   Prob(JB):                         0.00\nKurtosis:                      46.440   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n=== Probit Regression ===\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                50083\nModel:                         Probit   Df Residuals:                    50081\nMethod:                           MLE   Df Model:                            1\nDate:                Wed, 23 Apr 2025   Pseudo R-squ.:               0.0009783\nTime:                        16:11:57   Log-Likelihood:                -5030.5\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                  0.001696\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -2.1001      0.023    -90.073      0.000      -2.146      -2.054\ntreatment      0.0868      0.028      3.113      0.002       0.032       0.141\n==============================================================================\n\n\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.formula.api as smf\nfrom scipy.stats import ttest_ind\n\n# Keep only treated individuals for ratio comparisons\ndf_treat = df[df['treatment'] == 1].copy()\n\n# Create ratio1 dummy (1 if 1:1 match ratio)\ndf_treat['ratio1'] = (df_treat['ratio'] == \"1\").astype(int)\n\n# --- T-tests: comparing response rates between ratio levels ---\ndef ttest_response(var1, var2, label1, label2):\n    g1 = df_treat[df_treat[var1] == 1]['gave']\n    g2 = df_treat[df_treat[var2] == 1]['gave']\n    t_stat, p_val = ttest_ind(g1, g2, equal_var=False)\n    print(f\"T-test: {label1} vs {label2} | t = {t_stat:.3f}, p = {p_val:.4f}\")\n\n# Run t-tests between match ratios\nttest_response('ratio1', 'ratio2', '1:1', '2:1')\nttest_response('ratio2', 'ratio3', '2:1', '3:1')\nttest_response('ratio1', 'ratio3', '1:1', '3:1')\n\n# --- Regression: gave ~ ratio1 + ratio2 + ratio3 (1:1 is omitted baseline) ---\nreg1 = smf.ols(\"gave ~ ratio2 + ratio3\", data=df_treat).fit()\n\n# --- Alternatively: use categorical variable for ratio ---\nreg2 = smf.ols(\"gave ~ C(ratio)\", data=df_treat).fit()\n\n# --- Print regression summaries ---\nprint(\"\\nRegression with dummy variables (1:1 omitted):\")\nprint(reg1.summary())\n\nprint(\"\\nRegression with categorical ratio variable:\")\nprint(reg2.summary())\n\n# --- Response rate differences from raw data ---\nrate_1_1 = df_treat[df_treat['ratio'] == \"1\"]['gave'].mean()\nrate_2_1 = df_treat[df_treat['ratio'] == \"2\"]['gave'].mean()\nrate_3_1 = df_treat[df_treat['ratio'] == \"3\"]['gave'].mean()\n\ndiff_2_1_vs_1_1 = rate_2_1 - rate_1_1\ndiff_3_1_vs_2_1 = rate_3_1 - rate_2_1\n\nprint(f\"\\nResponse Rate Differences from Data:\")\nprint(f\"2:1 - 1:1 = {diff_2_1_vs_1_1:.4f}\")\nprint(f\"3:1 - 2:1 = {diff_3_1_vs_2_1:.4f}\")\n\n# --- Compare to regression coefficients ---\ncoef_2_1 = reg1.params['ratio2']\ncoef_3_1 = reg1.params['ratio3']\ndiff_coef_3_1_vs_2_1 = coef_3_1 - coef_2_1\n\nprint(f\"\\nResponse Rate Differences from Regression Coefficients:\")\nprint(f\"2:1 - 1:1 = {coef_2_1:.4f}\")\nprint(f\"3:1 - 2:1 = {diff_coef_3_1_vs_2_1:.4f}\")\n\nT-test: 1:1 vs 2:1 | t = nan, p = nan\nT-test: 2:1 vs 3:1 | t = -0.050, p = 0.9600\nT-test: 1:1 vs 3:1 | t = nan, p = nan\n\nRegression with dummy variables (1:1 omitted):\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                    0.6454\nDate:                Wed, 23 Apr 2025   Prob (F-statistic):              0.524\nTime:                        16:11:57   Log-Likelihood:                 16688.\nNo. Observations:               33396   AIC:                        -3.337e+04\nDf Residuals:                   33393   BIC:                        -3.334e+04\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.0207      0.001     14.912      0.000       0.018       0.023\nratio2         0.0019      0.002      0.958      0.338      -0.002       0.006\nratio3         0.0020      0.002      1.008      0.313      -0.002       0.006\n==============================================================================\nOmnibus:                    38963.957   Durbin-Watson:                   1.995\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          2506478.937\nSkew:                           6.511   Prob(JB):                         0.00\nKurtosis:                      43.394   Cond. No.                         3.73\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\nRegression with categorical ratio variable:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                    0.4263\nDate:                Wed, 23 Apr 2025   Prob (F-statistic):              0.734\nTime:                        16:11:57   Log-Likelihood:                 16688.\nNo. Observations:               33396   AIC:                        -3.337e+04\nDf Residuals:                   33392   BIC:                        -3.333e+04\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept       1.23e+09   1.12e+10      0.110      0.912   -2.07e+10    2.32e+10\nC(ratio)[T.1]  -1.23e+09   1.12e+10     -0.110      0.912   -2.32e+10    2.07e+10\nC(ratio)[T.2]  -1.23e+09   1.12e+10     -0.110      0.912   -2.32e+10    2.07e+10\nC(ratio)[T.3]  -1.23e+09   1.12e+10     -0.110      0.912   -2.32e+10    2.07e+10\n==============================================================================\nOmnibus:                    38963.855   Durbin-Watson:                   1.995\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          2506451.717\nSkew:                           6.511   Prob(JB):                         0.00\nKurtosis:                      43.394   Cond. No.                     3.22e+13\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The smallest eigenvalue is 4.31e-23. This might indicate that there are\nstrong multicollinearity problems or that the design matrix is singular.\n\nResponse Rate Differences from Data:\n2:1 - 1:1 = nan\n3:1 - 2:1 = nan\n\nResponse Rate Differences from Regression Coefficients:\n2:1 - 1:1 = 0.0019\n3:1 - 2:1 = 0.0001\n\n\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\nWe explore whether being offered a matching grant affects: 1. How much people donate on average (unconditionally) 2. How much people donate among those who do give (conditionally) 3. The distribution of donations with annotated averages\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\n\n# --- Unconditional regression (all people) ---\nmodel_uncond = smf.ols(\"amount ~ treatment\", data=df).fit()\nprint(\"Unconditional OLS regression (donation amount on treatment):\")\nprint(model_uncond.summary())\n\n# --- Conditional regression (only among donors) ---\ndf_donors = df[df['gave'] == 1]\nmodel_cond = smf.ols(\"amount ~ treatment\", data=df_donors).fit()\nprint(\"\\nConditional OLS regression (amount | gave == 1):\")\nprint(model_cond.summary())\n\n# --- Interpretation prompt ---\nprint(\"\\nInterpretation:\")\nprint(\"- The unconditional regression includes everyone and reflects both intensive (amount) and extensive (whether they donated) margins.\")\nprint(\"- The conditional regression isolates only the intensive margin — how much donors gave once they decided to give.\")\nprint(\"- The treatment coefficient in the conditional regression does NOT have a direct causal interpretation; it's conditional on post-treatment behavior.\")\n\n# --- Histograms of donation amounts among donors, by treatment ---\ntreat_donors = df_donors[df_donors['treatment'] == 1]['amount']\ncontrol_donors = df_donors[df_donors['treatment'] == 0]['amount']\n\nplt.figure(figsize=(14, 6))\n\n# Control group\nplt.subplot(1, 2, 1)\nplt.hist(control_donors, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\nplt.axvline(control_donors.mean(), color='red', linestyle='dashed', linewidth=2)\nplt.title('Control Group: Donation Amounts (Among Donors)')\nplt.xlabel('Amount Donated')\nplt.ylabel('Frequency')\nplt.text(control_donors.mean(), plt.ylim()[1]*0.9, f\"Mean: {control_donors.mean():.2f}\", color='red')\n\n# Treatment group\nplt.subplot(1, 2, 2)\nplt.hist(treat_donors, bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\nplt.axvline(treat_donors.mean(), color='red', linestyle='dashed', linewidth=2)\nplt.title('Treatment Group: Donation Amounts (Among Donors)')\nplt.xlabel('Amount Donated')\nplt.ylabel('Frequency')\nplt.text(treat_donors.mean(), plt.ylim()[1]*0.9, f\"Mean: {treat_donors.mean():.2f}\", color='red')\n\nplt.tight_layout()\nplt.show()\n\nUnconditional OLS regression (donation amount on treatment):\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     3.461\nDate:                Wed, 23 Apr 2025   Prob (F-statistic):             0.0628\nTime:                        16:11:57   Log-Likelihood:            -1.7946e+05\nNo. Observations:               50083   AIC:                         3.589e+05\nDf Residuals:                   50081   BIC:                         3.589e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.8133      0.067     12.063      0.000       0.681       0.945\ntreatment      0.1536      0.083      1.861      0.063      -0.008       0.315\n==============================================================================\nOmnibus:                    96861.113   Durbin-Watson:                   2.008\nProb(Omnibus):                  0.000   Jarque-Bera (JB):        240735713.635\nSkew:                          15.297   Prob(JB):                         0.00\nKurtosis:                     341.269   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\nConditional OLS regression (amount | gave == 1):\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.001\nMethod:                 Least Squares   F-statistic:                    0.3374\nDate:                Wed, 23 Apr 2025   Prob (F-statistic):              0.561\nTime:                        16:11:57   Log-Likelihood:                -5326.8\nNo. Observations:                1034   AIC:                         1.066e+04\nDf Residuals:                    1032   BIC:                         1.067e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     45.5403      2.423     18.792      0.000      40.785      50.296\ntreatment     -1.6684      2.872     -0.581      0.561      -7.305       3.968\n==============================================================================\nOmnibus:                      587.258   Durbin-Watson:                   2.031\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             5623.279\nSkew:                           2.464   Prob(JB):                         0.00\nKurtosis:                      13.307   Cond. No.                         3.49\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\nInterpretation:\n- The unconditional regression includes everyone and reflects both intensive (amount) and extensive (whether they donated) margins.\n- The conditional regression isolates only the intensive margin — how much donors gave once they decided to give.\n- The treatment coefficient in the conditional regression does NOT have a direct causal interpretation; it's conditional on post-treatment behavior."
  },
  {
    "objectID": "projects/HW1/hw1_questions.html#simulation-experiment",
    "href": "projects/HW1/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nTo simulate the intuition behind comparing group means, we draw: - 100,000 samples from the control group donation distribution - 10,000 samples from the treatment group We compute the difference between paired samples and plot the cumulative average of the differences.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Use unconditional donation amount distributions\ncontrol_dist = df[df['treatment'] == 0]['amount']\ntreatment_dist = df[df['treatment'] == 1]['amount']\n\n# Simulate draws\nnp.random.seed(42)\nsim_control = np.random.choice(control_dist, size=100000, replace=True)\nsim_treat = np.random.choice(treatment_dist, size=10000, replace=True)\n\n# Match lengths for subtraction (repeat treatment sample)\nsim_treat_matched = np.tile(sim_treat, 10)  # Now length = 100000\n\n# Compute vector of differences\ndiffs = sim_treat_matched - sim_control\n\n# Compute cumulative average\ncumulative_avg = np.cumsum(diffs) / np.arange(1, len(diffs) + 1)\n\n# True difference in means\ntrue_diff = treatment_dist.mean() - control_dist.mean()\n\n# Plot\nplt.figure(figsize=(12, 6))\nplt.plot(cumulative_avg, label='Cumulative Average of Differences')\nplt.axhline(true_diff, color='red', linestyle='--', linewidth=2, label='True Mean Difference')\nplt.xlabel('Number of Samples')\nplt.ylabel('Cumulative Average (Treatment - Control)')\nplt.title('Simulated Cumulative Average of Donation Differences')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Display true difference for interpretation\nprint(f\"\\nTrue difference in average donation amount (treatment - control): {true_diff:.4f}\")\n\n\n\n\n\n\n\n\n\nTrue difference in average donation amount (treatment - control): 0.1536\n\n\n\n\nCentral Limit Theorem\nWe simulate 1000 experiments at each sample size (50, 200, 500, 1000). Each experiment: - Takes independent samples from control and treatment donation distributions - Computes the mean difference in donations We then plot histograms of these mean differences and observe whether zero falls near the center or in the tails.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Extract full distributions\ncontrol = df[df['treatment'] == 0]['amount'].values\ntreatment = df[df['treatment'] == 1]['amount'].values\n\n# Sample sizes to simulate\nsample_sizes = [50, 200, 500, 1000]\nn_simulations = 1000\n\n# Prepare for plots\nfig, axs = plt.subplots(2, 2, figsize=(14, 10))\naxs = axs.flatten()\n\n# Run simulation and plotting loop\nnp.random.seed(123)\nfor i, n in enumerate(sample_sizes):\n    diff_means = []\n    for _ in range(n_simulations):\n        sample_control = np.random.choice(control, n, replace=True)\n        sample_treatment = np.random.choice(treatment, n, replace=True)\n        diff_means.append(sample_treatment.mean() - sample_control.mean())\n    \n    axs[i].hist(diff_means, bins=30, color='skyblue', edgecolor='black')\n    axs[i].axvline(0, color='red', linestyle='--', linewidth=2, label=\"Zero\")\n    axs[i].set_title(f\"Sample Size = {n}\")\n    axs[i].set_xlabel(\"Average Difference (Treatment - Control)\")\n    axs[i].set_ylabel(\"Frequency\")\n    axs[i].legend()\n\nplt.suptitle(\"Distributions of Average Differences from 1000 Simulated Experiments\")\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()"
  },
  {
    "objectID": "projects/HW1/hw1_questions.html#load-and-describe-dataset",
    "href": "projects/HW1/hw1_questions.html#load-and-describe-dataset",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Load and Describe Dataset",
    "text": "Load and Describe Dataset\n\nimport pandas as pd\n\n# Load the dataset\ndata_path = \"karlan_list_2007.dta\"\ndf = pd.read_stata(data_path)\n\n# Identify variable types\nnumerical_vars = df.select_dtypes(include=['number']).columns.tolist()\ncategorical_vars = df.select_dtypes(include=['category']).columns.tolist()\n\n# Summary statistics for numerical variables\nsummary_stats = df.describe()\n\n# Display variable types and summary\nprint(\"Numerical Variables:\")\nfor var in numerical_vars:\n    print(f\" - {var}\")\n\nprint(\"\\nCategorical Variables:\")\nfor var in categorical_vars:\n    print(f\" - {var}\")\n\nprint(\"\\nSummary Statistics for Numerical Variables:\")\nprint(summary_stats)\n\nNumerical Variables:\n - treatment\n - control\n - ratio2\n - ratio3\n - size25\n - size50\n - size100\n - sizeno\n - askd1\n - askd2\n - askd3\n - ask1\n - ask2\n - ask3\n - amount\n - gave\n - amountchange\n - hpa\n - ltmedmra\n - freq\n - years\n - year5\n - mrm2\n - dormant\n - female\n - couple\n - state50one\n - nonlit\n - cases\n - statecnt\n - stateresponse\n - stateresponset\n - stateresponsec\n - stateresponsetminc\n - perbush\n - close25\n - red0\n - blue0\n - redcty\n - bluecty\n - pwhite\n - pblack\n - page18_39\n - ave_hh_sz\n - median_hhincome\n - powner\n - psch_atlstba\n - pop_propurban\n\nCategorical Variables:\n - ratio\n - size\n - ask\n\nSummary Statistics for Numerical Variables:\n          treatment       control        ratio2        ratio3        size25  \\\ncount  50083.000000  50083.000000  50083.000000  50083.000000  50083.000000   \nmean       0.666813      0.333187      0.222311      0.222211      0.166723   \nstd        0.471357      0.471357      0.415803      0.415736      0.372732   \nmin        0.000000      0.000000      0.000000      0.000000      0.000000   \n25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n50%        1.000000      0.000000      0.000000      0.000000      0.000000   \n75%        1.000000      1.000000      0.000000      0.000000      0.000000   \nmax        1.000000      1.000000      1.000000      1.000000      1.000000   \n\n             size50       size100        sizeno         askd1         askd2  \\\ncount  50083.000000  50083.000000  50083.000000  50083.000000  50083.000000   \nmean       0.166623      0.166723      0.166743      0.222311      0.222291   \nstd        0.372643      0.372732      0.372750      0.415803      0.415790   \nmin        0.000000      0.000000      0.000000      0.000000      0.000000   \n25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n75%        0.000000      0.000000      0.000000      0.000000      0.000000   \nmax        1.000000      1.000000      1.000000      1.000000      1.000000   \n\n       ...        redcty       bluecty        pwhite        pblack  \\\ncount  ...  49978.000000  49978.000000  48217.000000  48047.000000   \nmean   ...      0.510245      0.488715      0.819599      0.086710   \nstd    ...      0.499900      0.499878      0.168560      0.135868   \nmin    ...      0.000000      0.000000      0.009418      0.000000   \n25%    ...      0.000000      0.000000      0.755845      0.014729   \n50%    ...      1.000000      0.000000      0.872797      0.036554   \n75%    ...      1.000000      1.000000      0.938827      0.090882   \nmax    ...      1.000000      1.000000      1.000000      0.989622   \n\n          page18_39     ave_hh_sz  median_hhincome        powner  \\\ncount  48217.000000  48221.000000     48209.000000  48214.000000   \nmean       0.321694      2.429012     54815.700533      0.669418   \nstd        0.103039      0.378105     22027.316665      0.193405   \nmin        0.000000      0.000000      5000.000000      0.000000   \n25%        0.258311      2.210000     39181.000000      0.560222   \n50%        0.305534      2.440000     50673.000000      0.712296   \n75%        0.369132      2.660000     66005.000000      0.816798   \nmax        0.997544      5.270000    200001.000000      1.000000   \n\n       psch_atlstba  pop_propurban  \ncount  48215.000000   48217.000000  \nmean       0.391661       0.871968  \nstd        0.186599       0.258633  \nmin        0.000000       0.000000  \n25%        0.235647       0.884929  \n50%        0.373744       1.000000  \n75%        0.530036       1.000000  \nmax        1.000000       1.000000  \n\n[8 rows x 48 columns]\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\nIn this section, we check if the variable mrm2 (months since last donation) is balanced across treatment and control groups. We do this using:\n\nA Welch’s t-test using the formula from class\nA linear regression of mrm2 on treatment\n\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.formula.api as smf\nfrom scipy import stats\n\n# Filter for valid mrm2 values\ndf_mrm2 = df[['mrm2', 'treatment']].dropna()\n\n# Split groups\ntreat = df_mrm2[df_mrm2['treatment'] == 1]['mrm2']\ncontrol = df_mrm2[df_mrm2['treatment'] == 0]['mrm2']\n\n# Sample statistics\nx1, x2 = treat.mean(), control.mean()\ns1, s2 = treat.std(ddof=1), control.std(ddof=1)\nn1, n2 = len(treat), len(control)\n\n# Welch's t-test manually (from slide 37)\nse_diff = np.sqrt((s1**2 / n1) + (s2**2 / n2))\nt_stat = (x1 - x2) / se_diff\n\n# Degrees of freedom (Welch–Satterthwaite)\ndf_num = (s1**2 / n1 + s2**2 / n2)**2\ndf_denom = ((s1**2 / n1)**2 / (n1 - 1)) + ((s2**2 / n2)**2 / (n2 - 1))\ndf_welch = df_num / df_denom\n\n# Two-sided p-value\np_val_ttest = 2 * stats.t.sf(np.abs(t_stat), df_welch)\n\n# Linear regression\nmodel = smf.ols('mrm2 ~ treatment', data=df_mrm2).fit()\n\n# Show results\nprint(f\"Welch's t-statistic: {t_stat:.3f}\")\nprint(f\"Welch's p-value: {p_val_ttest:.3f}\")\nprint(f\"Regression coefficient (treatment effect): {model.params['treatment']:.3f}\")\nprint(f\"Regression t-statistic: {model.tvalues['treatment']:.3f}\")\nprint(f\"Regression p-value: {model.pvalues['treatment']:.3f}\")\n\nWelch's t-statistic: 0.120\nWelch's p-value: 0.905\nRegression coefficient (treatment effect): 0.014\nRegression t-statistic: 0.119\nRegression p-value: 0.905"
  },
  {
    "objectID": "hw1.html",
    "href": "hw1.html",
    "title": "Nadifa's Awesome Work",
    "section": "",
    "text": "import pandas as pd\n\n# Load the dataset\ndata_path = \"karlan_list_2007.dta\"\ndf = pd.read_stata(data_path)\n\n# Dictionary of variable descriptions\ndescriptions = {\n    \"treatment\": \"Treatment\",\n    \"control\": \"Control\",\n    \"ratio\": \"Match ratio\",\n    \"ratio2\": \"2:1 match ratio\",\n    \"ratio3\": \"3:1 match ratio\",\n    \"size\": \"Match threshold\",\n    \"size25\": \"$25,000 match threshold\",\n    \"size50\": \"$50,000 match threshold\",\n    \"size100\": \"$100,000 match threshold\",\n    \"sizeno\": \"Unstated match threshold\",\n    \"ask\": \"Suggested donation amount\",\n    \"askd1\": \"Suggested donation was highest previous contribution\",\n    \"askd2\": \"Suggested donation was 1.25 x highest previous contribution\",\n    \"askd3\": \"Suggested donation was 1.50 x highest previous contribution\",\n    \"ask1\": \"Highest previous contribution (for suggestion)\",\n    \"ask2\": \"1.25 x highest previous contribution (for suggestion)\",\n    \"ask3\": \"1.50 x highest previous contribution (for suggestion)\",\n    \"amount\": \"Dollars given\",\n    \"gave\": \"Gave anything\",\n    \"amountchange\": \"Change in amount given\",\n    \"hpa\": \"Highest previous contribution\",\n    \"ltmedmra\": \"Small prior donor: last gift was less than median $35\",\n    \"freq\": \"Number of prior donations\",\n    \"years\": \"Number of years since initial donation\",\n    \"year5\": \"At least 5 years since initial donation\",\n    \"mrm2\": \"Number of months since last donation\",\n    \"dormant\": \"Already donated in 2005\",\n    \"female\": \"Female\",\n    \"couple\": \"Couple\",\n    \"state50one\": \"State tag: 1 for one observation of each of 50 states; 0 otherwise\",\n    \"nonlit\": \"Nonlitigation\",\n    \"cases\": \"Court cases from state in 2004-5 in which organization was involved\",\n    \"statecnt\": \"Percent of sample from state\",\n    \"stateresponse\": \"Proportion of sample from the state who gave\",\n    \"stateresponset\": \"Proportion of treated sample from the state who gave\",\n    \"stateresponsec\": \"Proportion of control sample from the state who gave\",\n    \"stateresponsetminc\": \"stateresponset - stateresponsec\",\n    \"perbush\": \"State vote share for Bush\",\n    \"close25\": \"State vote share for Bush between 47.5% and 52.5%\",\n    \"red0\": \"Red state\",\n    \"blue0\": \"Blue state\",\n    \"redcty\": \"Red county\",\n    \"bluecty\": \"Blue county\",\n    \"pwhite\": \"Proportion white within zip code\",\n    \"pblack\": \"Proportion black within zip code\",\n    \"page18_39\": \"Proportion age 18-39 within zip code\",\n    \"ave_hh_sz\": \"Average household size within zip code\",\n    \"median_hhincome\": \"Median household income within zip code\",\n    \"powner\": \"Proportion house owner within zip code\",\n    \"psch_atlstba\": \"Proportion who finished college within zip code\",\n    \"pop_propurban\": \"Proportion of population urban within zip code\"\n}\n\nprint(\"\\nVariable Descriptions:\")\nfor var, desc in descriptions.items():\n    print(f\" - `{var}`: {desc}\")\n# Identify variable types\nnumerical_vars = df.select_dtypes(include=['number']).columns.tolist()\ncategorical_vars = df.select_dtypes(include=['category']).columns.tolist()\n\n# Summary statistics for numerical variables\nsummary_stats = df.describe()\n\n# Display variable types and summary\nprint(\"Numerical Variables:\")\nfor var in numerical_vars:\n    print(f\" - {var}\")\n\nprint(\"\\nCategorical Variables:\")\nfor var in categorical_vars:\n    print(f\" - {var}\")\n\nprint(\"\\nSummary Statistics for Numerical Variables:\")\nprint(summary_stats)\n\n\n\n\nVariable Descriptions:\n - `treatment`: Treatment\n - `control`: Control\n - `ratio`: Match ratio\n - `ratio2`: 2:1 match ratio\n - `ratio3`: 3:1 match ratio\n - `size`: Match threshold\n - `size25`: $25,000 match threshold\n - `size50`: $50,000 match threshold\n - `size100`: $100,000 match threshold\n - `sizeno`: Unstated match threshold\n - `ask`: Suggested donation amount\n - `askd1`: Suggested donation was highest previous contribution\n - `askd2`: Suggested donation was 1.25 x highest previous contribution\n - `askd3`: Suggested donation was 1.50 x highest previous contribution\n - `ask1`: Highest previous contribution (for suggestion)\n - `ask2`: 1.25 x highest previous contribution (for suggestion)\n - `ask3`: 1.50 x highest previous contribution (for suggestion)\n - `amount`: Dollars given\n - `gave`: Gave anything\n - `amountchange`: Change in amount given\n - `hpa`: Highest previous contribution\n - `ltmedmra`: Small prior donor: last gift was less than median $35\n - `freq`: Number of prior donations\n - `years`: Number of years since initial donation\n - `year5`: At least 5 years since initial donation\n - `mrm2`: Number of months since last donation\n - `dormant`: Already donated in 2005\n - `female`: Female\n - `couple`: Couple\n - `state50one`: State tag: 1 for one observation of each of 50 states; 0 otherwise\n - `nonlit`: Nonlitigation\n - `cases`: Court cases from state in 2004-5 in which organization was involved\n - `statecnt`: Percent of sample from state\n - `stateresponse`: Proportion of sample from the state who gave\n - `stateresponset`: Proportion of treated sample from the state who gave\n - `stateresponsec`: Proportion of control sample from the state who gave\n - `stateresponsetminc`: stateresponset - stateresponsec\n - `perbush`: State vote share for Bush\n - `close25`: State vote share for Bush between 47.5% and 52.5%\n - `red0`: Red state\n - `blue0`: Blue state\n - `redcty`: Red county\n - `bluecty`: Blue county\n - `pwhite`: Proportion white within zip code\n - `pblack`: Proportion black within zip code\n - `page18_39`: Proportion age 18-39 within zip code\n - `ave_hh_sz`: Average household size within zip code\n - `median_hhincome`: Median household income within zip code\n - `powner`: Proportion house owner within zip code\n - `psch_atlstba`: Proportion who finished college within zip code\n - `pop_propurban`: Proportion of population urban within zip code\nNumerical Variables:\n - treatment\n - control\n - ratio2\n - ratio3\n - size25\n - size50\n - size100\n - sizeno\n - askd1\n - askd2\n - askd3\n - ask1\n - ask2\n - ask3\n - amount\n - gave\n - amountchange\n - hpa\n - ltmedmra\n - freq\n - years\n - year5\n - mrm2\n - dormant\n - female\n - couple\n - state50one\n - nonlit\n - cases\n - statecnt\n - stateresponse\n - stateresponset\n - stateresponsec\n - stateresponsetminc\n - perbush\n - close25\n - red0\n - blue0\n - redcty\n - bluecty\n - pwhite\n - pblack\n - page18_39\n - ave_hh_sz\n - median_hhincome\n - powner\n - psch_atlstba\n - pop_propurban\n\nCategorical Variables:\n - ratio\n - size\n - ask\n\nSummary Statistics for Numerical Variables:\n          treatment       control        ratio2        ratio3        size25  \\\ncount  50083.000000  50083.000000  50083.000000  50083.000000  50083.000000   \nmean       0.666813      0.333187      0.222311      0.222211      0.166723   \nstd        0.471357      0.471357      0.415803      0.415736      0.372732   \nmin        0.000000      0.000000      0.000000      0.000000      0.000000   \n25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n50%        1.000000      0.000000      0.000000      0.000000      0.000000   \n75%        1.000000      1.000000      0.000000      0.000000      0.000000   \nmax        1.000000      1.000000      1.000000      1.000000      1.000000   \n\n             size50       size100        sizeno         askd1         askd2  \\\ncount  50083.000000  50083.000000  50083.000000  50083.000000  50083.000000   \nmean       0.166623      0.166723      0.166743      0.222311      0.222291   \nstd        0.372643      0.372732      0.372750      0.415803      0.415790   \nmin        0.000000      0.000000      0.000000      0.000000      0.000000   \n25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n75%        0.000000      0.000000      0.000000      0.000000      0.000000   \nmax        1.000000      1.000000      1.000000      1.000000      1.000000   \n\n       ...        redcty       bluecty        pwhite        pblack  \\\ncount  ...  49978.000000  49978.000000  48217.000000  48047.000000   \nmean   ...      0.510245      0.488715      0.819599      0.086710   \nstd    ...      0.499900      0.499878      0.168560      0.135868   \nmin    ...      0.000000      0.000000      0.009418      0.000000   \n25%    ...      0.000000      0.000000      0.755845      0.014729   \n50%    ...      1.000000      0.000000      0.872797      0.036554   \n75%    ...      1.000000      1.000000      0.938827      0.090882   \nmax    ...      1.000000      1.000000      1.000000      0.989622   \n\n          page18_39     ave_hh_sz  median_hhincome        powner  \\\ncount  48217.000000  48221.000000     48209.000000  48214.000000   \nmean       0.321694      2.429012     54815.700533      0.669418   \nstd        0.103039      0.378105     22027.316665      0.193405   \nmin        0.000000      0.000000      5000.000000      0.000000   \n25%        0.258311      2.210000     39181.000000      0.560222   \n50%        0.305534      2.440000     50673.000000      0.712296   \n75%        0.369132      2.660000     66005.000000      0.816798   \nmax        0.997544      5.270000    200001.000000      1.000000   \n\n       psch_atlstba  pop_propurban  \ncount  48215.000000   48217.000000  \nmean       0.391661       0.871968  \nstd        0.186599       0.258633  \nmin        0.000000       0.000000  \n25%        0.235647       0.884929  \n50%        0.373744       1.000000  \n75%        0.530036       1.000000  \nmax        1.000000       1.000000  \n\n[8 rows x 48 columns]\n\n\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.formula.api as smf\nfrom scipy import stats\n\n# Load dataset\ndf = pd.read_stata(\"projects/HW1/karlan_list_2007.dta\")\n\n# Filter for valid mrm2 values\ndf_mrm2 = df[['mrm2', 'treatment']].dropna()\n\n# Split groups\ntreat = df_mrm2[df_mrm2['treatment'] == 1]['mrm2']\ncontrol = df_mrm2[df_mrm2['treatment'] == 0]['mrm2']\n\n# Sample statistics\nx1, x2 = treat.mean(), control.mean()\ns1, s2 = treat.std(ddof=1), control.std(ddof=1)\nn1, n2 = len(treat), len(control)\n\n# Welch's t-test manually (from slide 37)\nse_diff = np.sqrt((s1**2 / n1) + (s2**2 / n2))\nt_stat = (x1 - x2) / se_diff\n\n# Degrees of freedom (Welch–Satterthwaite)\ndf_num = (s1**2 / n1 + s2**2 / n2)**2\ndf_denom = ((s1**2 / n1)**2 / (n1 - 1)) + ((s2**2 / n2)**2 / (n2 - 1))\ndf_welch = df_num / df_denom\n\n# Two-sided p-value\np_val_ttest = 2 * stats.t.sf(np.abs(t_stat), df_welch)\n\n# Linear regression\nmodel = smf.ols('mrm2 ~ treatment', data=df_mrm2).fit()\n\n# Show results\nprint(f\"Welch's t-statistic: {t_stat:.3f}\")\nprint(f\"Welch's p-value: {p_val_ttest:.3f}\")\nprint(f\"Regression coefficient (treatment effect): {model.params['treatment']:.3f}\")\nprint(f\"Regression t-statistic: {model.tvalues['treatment']:.3f}\")\nprint(f\"Regression p-value: {model.pvalues['treatment']:.3f}\")\n\nWelch's t-statistic: 0.120\nWelch's p-value: 0.905\nRegression coefficient (treatment effect): 0.014\nRegression t-statistic: 0.119\nRegression p-value: 0.905"
  },
  {
    "objectID": "projects/HW1/hw1_questions.html#randomization-check-testing-baseline-balance",
    "href": "projects/HW1/hw1_questions.html#randomization-check-testing-baseline-balance",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Randomization Check: Testing Baseline Balance",
    "text": "Randomization Check: Testing Baseline Balance\nIn this section, we check if the variable mrm2 (months since last donation) is balanced across treatment and control groups. We do this using:\n\nA Welch’s t-test using the formula from class\nA linear regression of mrm2 on treatment\n\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.formula.api as smf\nfrom scipy import stats\n\n# Load dataset\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\n# Filter for valid mrm2 values\ndf_mrm2 = df[['mrm2', 'treatment']].dropna()\n\n# Split groups\ntreat = df_mrm2[df_mrm2['treatment'] == 1]['mrm2']\ncontrol = df_mrm2[df_mrm2['treatment'] == 0]['mrm2']\n\n# Sample statistics\nx1, x2 = treat.mean(), control.mean()\ns1, s2 = treat.std(ddof=1), control.std(ddof=1)\nn1, n2 = len(treat), len(control)\n\n# Welch's t-test manually (from slide 37)\nse_diff = np.sqrt((s1**2 / n1) + (s2**2 / n2))\nt_stat = (x1 - x2) / se_diff\n\n# Degrees of freedom (Welch–Satterthwaite)\ndf_num = (s1**2 / n1 + s2**2 / n2)**2\ndf_denom = ((s1**2 / n1)**2 / (n1 - 1)) + ((s2**2 / n2)**2 / (n2 - 1))\ndf_welch = df_num / df_denom\n\n# Two-sided p-value\np_val_ttest = 2 * stats.t.sf(np.abs(t_stat), df_welch)\n\n# Linear regression\nmodel = smf.ols('mrm2 ~ treatment', data=df_mrm2).fit()\n\n# Show results\nprint(f\"Welch's t-statistic: {t_stat:.3f}\")\nprint(f\"Welch's p-value: {p_val_ttest:.3f}\")\nprint(f\"Regression coefficient (treatment effect): {model.params['treatment']:.3f}\")\nprint(f\"Regression t-statistic: {model.tvalues['treatment']:.3f}\")\nprint(f\"Regression p-value: {model.pvalues['treatment']:.3f}\")\n\nWelch's t-statistic: 0.120\nWelch's p-value: 0.905\nRegression coefficient (treatment effect): 0.014\nRegression t-statistic: 0.119\nRegression p-value: 0.905"
  },
  {
    "objectID": "projects/HW1/hw1_questions.html#treatment-effects-on-donation-behavior",
    "href": "projects/HW1/hw1_questions.html#treatment-effects-on-donation-behavior",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Treatment Effects on Donation Behavior",
    "text": "Treatment Effects on Donation Behavior\nWe investigate whether being offered a matching grant (treatment) increased the likelihood of making a donation.\nWe’ll: - Visualize donation rates by group - Perform a Welch’s t-test and a linear regression - Run a probit regression to match results in Table 3, Column 1\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom scipy.stats import t\n\n# Load the data\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\n# --- Barplot: proportion who donated by treatment group ---\ndonation_rates = df.groupby('treatment')['gave'].mean()\n\nplt.figure(figsize=(10, 6))\nplt.bar(['Control', 'Treatment'], donation_rates.values)\nplt.title('Proportion of People Who Donated by Group')\nplt.ylabel('Proportion Donated')\nplt.ylim(0, 0.03)\nplt.grid(axis='y')\nplt.show()\n\n# --- Welch's t-test manually ---\ntreat = df[df['treatment'] == 1]['gave']\ncontrol = df[df['treatment'] == 0]['gave']\n\nx1, x2 = treat.mean(), control.mean()\ns1, s2 = treat.std(ddof=1), control.std(ddof=1)\nn1, n2 = len(treat), len(control)\n\nse = np.sqrt((s1**2 / n1) + (s2**2 / n2))\nt_stat = (x1 - x2) / se\ndf_num = (s1**2 / n1 + s2**2 / n2)**2\ndf_denom = ((s1**2 / n1)**2 / (n1 - 1)) + ((s2**2 / n2)**2 / (n2 - 1))\ndf_welch = df_num / df_denom\np_val_t = 2 * t.sf(np.abs(t_stat), df_welch)\n\n# --- Linear regression ---\nlm = smf.ols('gave ~ treatment', data=df).fit()\n\n# --- Probit regression ---\nprobit = smf.probit('gave ~ treatment', data=df).fit(disp=0)\n\n# --- Output summary ---\nprint(\"=== Barplot: Proportion Donated ===\")\nprint(donation_rates)\n\nprint(\"\\n=== Welch’s t-test ===\")\nprint(f\"t-statistic = {t_stat:.3f}, p-value = {p_val_t:.4f}\")\n\nprint(\"\\n=== Linear Regression ===\")\nprint(lm.summary())\n\nprint(\"\\n=== Probit Regression ===\")\nprint(probit.summary())\n\n\n\n\n\n\n\n\n=== Barplot: Proportion Donated ===\ntreatment\n0    0.017858\n1    0.022039\nName: gave, dtype: float64\n\n=== Welch’s t-test ===\nt-statistic = 3.209, p-value = 0.0013\n\n=== Linear Regression ===\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     9.618\nDate:                Wed, 23 Apr 2025   Prob (F-statistic):            0.00193\nTime:                        15:00:38   Log-Likelihood:                 26630.\nNo. Observations:               50083   AIC:                        -5.326e+04\nDf Residuals:                   50081   BIC:                        -5.324e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.0179      0.001     16.225      0.000       0.016       0.020\ntreatment      0.0042      0.001      3.101      0.002       0.002       0.007\n==============================================================================\nOmnibus:                    59814.280   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4317152.727\nSkew:                           6.740   Prob(JB):                         0.00\nKurtosis:                      46.440   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n=== Probit Regression ===\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                50083\nModel:                         Probit   Df Residuals:                    50081\nMethod:                           MLE   Df Model:                            1\nDate:                Wed, 23 Apr 2025   Pseudo R-squ.:               0.0009783\nTime:                        15:00:38   Log-Likelihood:                -5030.5\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                  0.001696\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -2.1001      0.023    -90.073      0.000      -2.146      -2.054\ntreatment      0.0868      0.028      3.113      0.002       0.032       0.141\n==============================================================================\n\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\ntodo: Use a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not. For example, does the 2:1 match rate lead increase the likelihood that someone donates as compared to the 1:1 match rate? Do your results support the “figures suggest” comment the authors make on page 8?\ntodo: Assess the same issue using a regression. Specifically, create the variable ratio1 then regress gave on ratio1, ratio2, and ratio3 (or alternatively, regress gave on the categorical variable ratio). Interpret the coefficients and their statistical precision.\ntodo: Calculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios. Do this directly from the data, and do it by computing the differences in the fitted coefficients of the previous regression. what do you conclude regarding the effectiveness of different sizes of matched donations?\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.formula.api as smf\nfrom scipy.stats import ttest_ind\n\n# Load the dataset\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\n# Keep only treated individuals for ratio comparisons\ndf_treat = df[df['treatment'] == 1].copy()\n\n# Create ratio1 dummy (1 if 1:1 match ratio)\ndf_treat['ratio1'] = (df_treat['ratio'] == \"1\").astype(int)\n\n# --- T-tests: comparing response rates between ratio levels ---\ndef ttest_response(var1, var2, label1, label2):\n    g1 = df_treat[df_treat[var1] == 1]['gave']\n    g2 = df_treat[df_treat[var2] == 1]['gave']\n    t_stat, p_val = ttest_ind(g1, g2, equal_var=False)\n    print(f\"T-test: {label1} vs {label2} | t = {t_stat:.3f}, p = {p_val:.4f}\")\n\n# Run t-tests between match ratios\nttest_response('ratio1', 'ratio2', '1:1', '2:1')\nttest_response('ratio2', 'ratio3', '2:1', '3:1')\nttest_response('ratio1', 'ratio3', '1:1', '3:1')\n\n# --- Regression: gave ~ ratio1 + ratio2 + ratio3 (1:1 is omitted baseline) ---\nreg1 = smf.ols(\"gave ~ ratio2 + ratio3\", data=df_treat).fit()\n\n# --- Alternatively: use categorical variable for ratio ---\nreg2 = smf.ols(\"gave ~ C(ratio)\", data=df_treat).fit()\n\n# --- Print regression summaries ---\nprint(\"\\nRegression with dummy variables (1:1 omitted):\")\nprint(reg1.summary())\n\nprint(\"\\nRegression with categorical ratio variable:\")\nprint(reg2.summary())\n\n# --- Response rate differences from raw data ---\nrate_1_1 = df_treat[df_treat['ratio'] == \"1\"]['gave'].mean()\nrate_2_1 = df_treat[df_treat['ratio'] == \"2\"]['gave'].mean()\nrate_3_1 = df_treat[df_treat['ratio'] == \"3\"]['gave'].mean()\n\ndiff_2_1_vs_1_1 = rate_2_1 - rate_1_1\ndiff_3_1_vs_2_1 = rate_3_1 - rate_2_1\n\nprint(f\"\\nResponse Rate Differences from Data:\")\nprint(f\"2:1 - 1:1 = {diff_2_1_vs_1_1:.4f}\")\nprint(f\"3:1 - 2:1 = {diff_3_1_vs_2_1:.4f}\")\n\n# --- Compare to regression coefficients ---\ncoef_2_1 = reg1.params['ratio2']\ncoef_3_1 = reg1.params['ratio3']\ndiff_coef_3_1_vs_2_1 = coef_3_1 - coef_2_1\n\nprint(f\"\\nResponse Rate Differences from Regression Coefficients:\")\nprint(f\"2:1 - 1:1 = {coef_2_1:.4f}\")\nprint(f\"3:1 - 2:1 = {diff_coef_3_1_vs_2_1:.4f}\")\n\nT-test: 1:1 vs 2:1 | t = nan, p = nan\nT-test: 2:1 vs 3:1 | t = -0.050, p = 0.9600\nT-test: 1:1 vs 3:1 | t = nan, p = nan\n\nRegression with dummy variables (1:1 omitted):\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                    0.6454\nDate:                Wed, 23 Apr 2025   Prob (F-statistic):              0.524\nTime:                        15:00:38   Log-Likelihood:                 16688.\nNo. Observations:               33396   AIC:                        -3.337e+04\nDf Residuals:                   33393   BIC:                        -3.334e+04\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.0207      0.001     14.912      0.000       0.018       0.023\nratio2         0.0019      0.002      0.958      0.338      -0.002       0.006\nratio3         0.0020      0.002      1.008      0.313      -0.002       0.006\n==============================================================================\nOmnibus:                    38963.957   Durbin-Watson:                   1.995\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          2506478.937\nSkew:                           6.511   Prob(JB):                         0.00\nKurtosis:                      43.394   Cond. No.                         3.73\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\nRegression with categorical ratio variable:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                    0.4263\nDate:                Wed, 23 Apr 2025   Prob (F-statistic):              0.734\nTime:                        15:00:38   Log-Likelihood:                 16688.\nNo. Observations:               33396   AIC:                        -3.337e+04\nDf Residuals:                   33392   BIC:                        -3.333e+04\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept       1.23e+09   1.12e+10      0.110      0.912   -2.07e+10    2.32e+10\nC(ratio)[T.1]  -1.23e+09   1.12e+10     -0.110      0.912   -2.32e+10    2.07e+10\nC(ratio)[T.2]  -1.23e+09   1.12e+10     -0.110      0.912   -2.32e+10    2.07e+10\nC(ratio)[T.3]  -1.23e+09   1.12e+10     -0.110      0.912   -2.32e+10    2.07e+10\n==============================================================================\nOmnibus:                    38963.855   Durbin-Watson:                   1.995\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          2506451.717\nSkew:                           6.511   Prob(JB):                         0.00\nKurtosis:                      43.394   Cond. No.                     3.22e+13\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The smallest eigenvalue is 4.31e-23. This might indicate that there are\nstrong multicollinearity problems or that the design matrix is singular.\n\nResponse Rate Differences from Data:\n2:1 - 1:1 = nan\n3:1 - 2:1 = nan\n\nResponse Rate Differences from Regression Coefficients:\n2:1 - 1:1 = 0.0019\n3:1 - 2:1 = 0.0001\n\n\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\ntodo: Calculate a t-test or run a bivariate linear regression of the donation amount on the treatment status. What do we learn from doing this analysis?\ntodo: now limit the data to just people who made a donation and repeat the previous analysis. This regression allows you to analyze how much respondents donate conditional on donating some positive amount. Interpret the regression coefficients – what did we learn? Does the treatment coefficient have a causal interpretation?\ntodo: Make two plot: one for the treatment group and one for the control. Each plot should be a histogram of the donation amounts only among people who donated. Add a red vertical bar or some other annotation to indicate the sample average for each plot."
  },
  {
    "objectID": "projects/HW1/hw1_questions.html#effect-of-treatment-on-donation-amount",
    "href": "projects/HW1/hw1_questions.html#effect-of-treatment-on-donation-amount",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Effect of Treatment on Donation Amount",
    "text": "Effect of Treatment on Donation Amount\nWe explore whether being offered a matching grant affects: 1. How much people donate on average (unconditionally) 2. How much people donate among those who do give (conditionally) 3. The distribution of donations with annotated averages\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\n\n# Load dataset\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\n# --- Unconditional regression (all people) ---\nmodel_uncond = smf.ols(\"amount ~ treatment\", data=df).fit()\nprint(\"Unconditional OLS regression (donation amount on treatment):\")\nprint(model_uncond.summary())\n\n# --- Conditional regression (only among donors) ---\ndf_donors = df[df['gave'] == 1]\nmodel_cond = smf.ols(\"amount ~ treatment\", data=df_donors).fit()\nprint(\"\\nConditional OLS regression (amount | gave == 1):\")\nprint(model_cond.summary())\n\n# --- Interpretation prompt ---\nprint(\"\\nInterpretation:\")\nprint(\"- The unconditional regression includes everyone and reflects both intensive (amount) and extensive (whether they donated) margins.\")\nprint(\"- The conditional regression isolates only the intensive margin — how much donors gave once they decided to give.\")\nprint(\"- The treatment coefficient in the conditional regression does NOT have a direct causal interpretation; it's conditional on post-treatment behavior.\")\n\n# --- Histograms of donation amounts among donors, by treatment ---\ntreat_donors = df_donors[df_donors['treatment'] == 1]['amount']\ncontrol_donors = df_donors[df_donors['treatment'] == 0]['amount']\n\nplt.figure(figsize=(14, 6))\n\n# Control group\nplt.subplot(1, 2, 1)\nplt.hist(control_donors, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\nplt.axvline(control_donors.mean(), color='red', linestyle='dashed', linewidth=2)\nplt.title('Control Group: Donation Amounts (Among Donors)')\nplt.xlabel('Amount Donated')\nplt.ylabel('Frequency')\nplt.text(control_donors.mean(), plt.ylim()[1]*0.9, f\"Mean: {control_donors.mean():.2f}\", color='red')\n\n# Treatment group\nplt.subplot(1, 2, 2)\nplt.hist(treat_donors, bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\nplt.axvline(treat_donors.mean(), color='red', linestyle='dashed', linewidth=2)\nplt.title('Treatment Group: Donation Amounts (Among Donors)')\nplt.xlabel('Amount Donated')\nplt.ylabel('Frequency')\nplt.text(treat_donors.mean(), plt.ylim()[1]*0.9, f\"Mean: {treat_donors.mean():.2f}\", color='red')\n\nplt.tight_layout()\nplt.show()\n\nUnconditional OLS regression (donation amount on treatment):\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     3.461\nDate:                Wed, 23 Apr 2025   Prob (F-statistic):             0.0628\nTime:                        15:00:38   Log-Likelihood:            -1.7946e+05\nNo. Observations:               50083   AIC:                         3.589e+05\nDf Residuals:                   50081   BIC:                         3.589e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.8133      0.067     12.063      0.000       0.681       0.945\ntreatment      0.1536      0.083      1.861      0.063      -0.008       0.315\n==============================================================================\nOmnibus:                    96861.113   Durbin-Watson:                   2.008\nProb(Omnibus):                  0.000   Jarque-Bera (JB):        240735713.635\nSkew:                          15.297   Prob(JB):                         0.00\nKurtosis:                     341.269   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\nConditional OLS regression (amount | gave == 1):\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.001\nMethod:                 Least Squares   F-statistic:                    0.3374\nDate:                Wed, 23 Apr 2025   Prob (F-statistic):              0.561\nTime:                        15:00:38   Log-Likelihood:                -5326.8\nNo. Observations:                1034   AIC:                         1.066e+04\nDf Residuals:                    1032   BIC:                         1.067e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     45.5403      2.423     18.792      0.000      40.785      50.296\ntreatment     -1.6684      2.872     -0.581      0.561      -7.305       3.968\n==============================================================================\nOmnibus:                      587.258   Durbin-Watson:                   2.031\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             5623.279\nSkew:                           2.464   Prob(JB):                         0.00\nKurtosis:                      13.307   Cond. No.                         3.49\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\nInterpretation:\n- The unconditional regression includes everyone and reflects both intensive (amount) and extensive (whether they donated) margins.\n- The conditional regression isolates only the intensive margin — how much donors gave once they decided to give.\n- The treatment coefficient in the conditional regression does NOT have a direct causal interpretation; it's conditional on post-treatment behavior."
  },
  {
    "objectID": "projects/HW1/hw1_questions.html#simulated-cumulative-differences",
    "href": "projects/HW1/hw1_questions.html#simulated-cumulative-differences",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulated Cumulative Differences",
    "text": "Simulated Cumulative Differences\nTo simulate the intuition behind comparing group means, we draw: - 100,000 samples from the control group donation distribution - 10,000 samples from the treatment group We compute the difference between paired samples and plot the cumulative average of the differences.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load dataset\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\n# Use unconditional donation amount distributions\ncontrol_dist = df[df['treatment'] == 0]['amount']\ntreatment_dist = df[df['treatment'] == 1]['amount']\n\n# Simulate draws\nnp.random.seed(42)\nsim_control = np.random.choice(control_dist, size=100000, replace=True)\nsim_treat = np.random.choice(treatment_dist, size=10000, replace=True)\n\n# Match lengths for subtraction (repeat treatment sample)\nsim_treat_matched = np.tile(sim_treat, 10)  # Now length = 100000\n\n# Compute vector of differences\ndiffs = sim_treat_matched - sim_control\n\n# Compute cumulative average\ncumulative_avg = np.cumsum(diffs) / np.arange(1, len(diffs) + 1)\n\n# True difference in means\ntrue_diff = treatment_dist.mean() - control_dist.mean()\n\n# Plot\nplt.figure(figsize=(12, 6))\nplt.plot(cumulative_avg, label='Cumulative Average of Differences')\nplt.axhline(true_diff, color='red', linestyle='--', linewidth=2, label='True Mean Difference')\nplt.xlabel('Number of Samples')\nplt.ylabel('Cumulative Average (Treatment - Control)')\nplt.title('Simulated Cumulative Average of Donation Differences')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Display true difference for interpretation\nprint(f\"\\nTrue difference in average donation amount (treatment - control): {true_diff:.4f}\")\n\n\n\n\n\n\n\n\n\nTrue difference in average donation amount (treatment - control): 0.1536\n\n\n\nCentral Limit Theorem\nto do: Make 4 histograms like those on slide 44 from our first class at sample sizes 50, 200, 500, and 1000 and explain these plots to the reader. To do this for a sample size of e.g. 50, take 50 draws from each of the control and treatment distributions, and calculate the average difference between those draws. Then repeat that process 999 more times so that you have 1000 averages. Plot the histogram of those averages. Comment on whether zero is in the “middle” of the distribution or whether it’s in the “tail.”"
  },
  {
    "objectID": "projects/HW1/hw1_questions.html#simulated-distribution-of-average-differences-at-varying-sample-sizes",
    "href": "projects/HW1/hw1_questions.html#simulated-distribution-of-average-differences-at-varying-sample-sizes",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulated Distribution of Average Differences at Varying Sample Sizes",
    "text": "Simulated Distribution of Average Differences at Varying Sample Sizes\nWe simulate 1000 experiments at each sample size (50, 200, 500, 1000). Each experiment: - Takes independent samples from control and treatment donation distributions - Computes the mean difference in donations We then plot histograms of these mean differences and observe whether zero falls near the center or in the tails.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load the dataset\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\n# Extract full distributions\ncontrol = df[df['treatment'] == 0]['amount'].values\ntreatment = df[df['treatment'] == 1]['amount'].values\n\n# Sample sizes to simulate\nsample_sizes = [50, 200, 500, 1000]\nn_simulations = 1000\n\n# Prepare for plots\nfig, axs = plt.subplots(2, 2, figsize=(14, 10))\naxs = axs.flatten()\n\n# Run simulation and plotting loop\nnp.random.seed(123)\nfor i, n in enumerate(sample_sizes):\n    diff_means = []\n    for _ in range(n_simulations):\n        sample_control = np.random.choice(control, n, replace=True)\n        sample_treatment = np.random.choice(treatment, n, replace=True)\n        diff_means.append(sample_treatment.mean() - sample_control.mean())\n    \n    axs[i].hist(diff_means, bins=30, color='skyblue', edgecolor='black')\n    axs[i].axvline(0, color='red', linestyle='--', linewidth=2, label=\"Zero\")\n    axs[i].set_title(f\"Sample Size = {n}\")\n    axs[i].set_xlabel(\"Average Difference (Treatment - Control)\")\n    axs[i].set_ylabel(\"Frequency\")\n    axs[i].legend()\n\nplt.suptitle(\"Distributions of Average Differences from 1000 Simulated Experiments\")\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()"
  }
]